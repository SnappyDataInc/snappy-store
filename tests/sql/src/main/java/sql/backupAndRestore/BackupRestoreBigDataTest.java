/*
 * Copyright (c) 2010-2015 Pivotal Software, Inc. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you
 * may not use this file except in compliance with the License. You
 * may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
 * implied. See the License for the specific language governing
 * permissions and limitations under the License. See accompanying
 * LICENSE file.
 */

package sql.backupAndRestore;

import java.io.File;
import java.io.IOException;
import java.sql.CallableStatement;
import java.sql.Clob;
import java.sql.Connection;
import java.sql.Date;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Properties;
import java.util.Set;
import java.util.TreeSet;
import java.util.concurrent.TimeUnit;

import com.gemstone.gemfire.LogWriter;

import hydra.DistributedSystemHelper;
import hydra.GsRandom;
import hydra.HostHelper;
import hydra.HydraRuntimeException;
import hydra.HydraThreadLocal;
import hydra.Log;
import hydra.MasterController;
import hydra.ProcessMgr;
import hydra.StopSchedulingOrder;
import hydra.StopSchedulingTaskOnClientOrder;
import hydra.TestConfig;
import hydra.blackboard.SharedCounters;
import hydra.gemfirexd.FabricServerHelper;
import sql.SQLHelper;
import sql.memscale.TableDefinition;
import sql.view.ViewTest;
import util.RandomValues;
import util.TestException;
import util.TestHelper;
import util.TestHelperPrms;

/**
 * The BackupRestoreBigDataTest is a scale test to exercise backup and restore in GemFireXD (no HDFS).
 * Basic test flow (after basic test initialization):
 *   1) Create random tables and their indexes and prepared statements,
 *   2) Populate all tables with some data (inserts)
 *   3) Perform a full backup,
 *   4) Client threads start performing random data operations (inserts, updates and deletes) being carefull to 'track'
 *      what operations are done do which keys,
 *   5) One of the Server threads will monitor how much data is being generated by the Client threads and perform
 *      incremental backups as needed and stop the test once a predetermined amount of data has been generated,
 *   6) Snapshot the data (prepare for validation),
 *   7) Stop GemFireXD and remove all data from the file system,
 *   8) Perform a restore of the data,
 *   9) Restart the GemFireXD,
 *  10) Validate the data.
 *
 * @author mpriest
 * @see ?
 * @since 1.4
 */
public class BackupRestoreBigDataTest extends ViewTest {

  private static sql.backupAndRestore.BackupRestoreBigDataTest BackupRestoreBigDataTest;

  private final static long MEGABYTE = 1024 * 1024;
  private final static long GIGABYTE = 1024 * 1024 * 1024;
  private static final Object KEY_TABLES = "tables";
  private static final Object KEY_TIMINGS = "timings";
  private static final Object KEY_TABLE_ROW_COUNTS = "tableRowCounts";
  private static final String KEY_VALIDATION_ERRORS = "validationErrors";
  private static final String SCHEMA_NAME = "bigDataTest";
  private static final String FILE_SEPARATOR = System.getProperty("file.separator");
  private static final String BACKUP_PREFIX = "backup_";
  private static final String BASELINE_OPT = "-baseline=";
  private static final String DISK_DIR_SUFFIX = "_disk";
  private static final String RESTORE_LINUX = "restore.sh";
  private static final String RESTORE_WINDOWS = "restore.bat";
  private static final String PREFIX_PK = "PK_";
  private static final String PREFIX_INSERT = "insert-";
  private static final String PREFIX_UPDATE = "update-This data was updated";
  private static final String THREADGROUP_CLIENT = "clientThreads";
  private static final String THREADGROUP_DDL = "ddlThread";

  private static final String DATA_TYPE_INTEGER = "INTEGER";
  private static final String DATA_TYPE_VARCHAR = "VARCHAR";
  private static final String DATA_TYPE_CLOB = "CLOB";
  private static final String DATA_TYPE_DATE = "DATE";

  private static LogWriter logWriter;
  private static Set<String> availableTables;
  private static Date insertDate;     // All new rows with Date fields should have this date
  private static Date updateDate;     // All updated rows with Date fields should have this date

  private static HydraThreadLocal threadLocal_clientThreadIsPaused = new HydraThreadLocal();
  private static HydraThreadLocal threadLocal_gfxdConnection = new HydraThreadLocal();
  private static HydraThreadLocal threadLocal_deletePreparedStatements = new HydraThreadLocal();
  private static HydraThreadLocal threadLocal_insertPreparedStatements = new HydraThreadLocal();
  private static HydraThreadLocal threadLocal_updatePreparedStatements = new HydraThreadLocal();
  private static HydraThreadLocal threadLocal_isClientLeader = new HydraThreadLocal();
  private static HydraThreadLocal threadLocal_opTracker = new HydraThreadLocal();

  /**
   * Use this to validate some of the test parameters before continuing with the test
   */
  public static void HydraTask_checkParameters() {
    // Check the sum of insertPercent, deletePercent, and updatePercent should total 100%
    int insertPercent = BackupAndRestorePrms.getInsertPercent();
    int updatePercent = BackupAndRestorePrms.getUpdatePercent();
    int deletePercent = BackupAndRestorePrms.getDeletePercent();
    int totalOpPercent = insertPercent + updatePercent + deletePercent;
    if (totalOpPercent != 100) {
      throw new TestException("The total operation percentage needs to equal 100%. The total operation percentage " +
                              "is the sum of the parameters:\n  sql.backupAndRestore.BackupAndRestorePrms-insertPercent (" +
                              insertPercent + "),\n  sql.backupAndRestore.BackupAndRestorePrms-updatePercent (" +
                              updatePercent + "), and\n  sql.backupAndRestore.BackupAndRestorePrms-deletePercent (" +
                              deletePercent + ")\n                                              which total " +
                              totalOpPercent + "%.");
    }
  }

  /**
   * Initialize the test. Make sure that we set the insert and update Date values in case the test runs over into a new day.
   */
  public static void HydraTask_initializeTest() {
    if (BackupRestoreBigDataTest == null) {
      BackupRestoreBigDataTest = new sql.backupAndRestore.BackupRestoreBigDataTest();

      // Set the insertDate to today's date (to be used in later validation)
      Calendar cal = Calendar.getInstance();
      insertDate = new Date(cal.getTime().getTime());
      // Set the updateDate to be tomorrow (to be used in later validation)
      cal.add(Calendar.DATE, 1);
      updateDate = new Date(cal.getTime().getTime());

      logWriter = Log.getLogWriter();
    }
  }

  /**
   * Use this to get a thin client driver connection to GemFireXD
   */
  public static void HydraTask_getGFXDConnection() {
    BackupRestoreBigDataTest.getGFXDConnection();
  }

  /**
   * Use this to get a thin client driver connection to GemFireXD
   */
  private void getGFXDConnection() {
    Connection conn = BackupRestoreBigDataTest.getGFXDClientConnection();
    logWriter.info("BackupRestoreBigDataTest.getGFXDConnection-Connection Set-conn=" + conn);
    threadLocal_gfxdConnection.set(conn);
  }

  /**
   * Hydra task to generate the sql for creating tables, but this task does not execute the sql. Generate random table
   * definitions and write them to the blackboard. This allows the test to use randomization to define tables, then each
   * client to create the tables can read from the blackboard so all clients can create the same tables.
   */
  public static void HydraTask_generateTableDefinitions() {
    // How many table definitions are we to generate
    int nbrTables = BackupAndRestorePrms.getNbrTables();
    if (nbrTables <= 0) {
      throw new TestException("Expected " + BackupAndRestorePrms.class.getName() + ".nbrTables to be > 0)");
    }
    List<TableDefinition> tableList = new ArrayList<TableDefinition>();

    int nbrLobColumnsThisTable, nbrColumnsThisTable;
    String tableName;
    TableDefinition tableDef;
    int nbrLobColumns = BackupAndRestorePrms.getNbrLobColumns();
    // Loop through each potential table definition
    for (int tableNbr = 1;tableNbr <= nbrTables;tableNbr++) {
      tableName = "Table_" + String.format("%03d", tableNbr);
      nbrColumnsThisTable = BackupAndRestorePrms.getNbrColumnsPerTable();
      logWriter.info("BackupRestoreBigDataTest.generateTableDefinitions-nbrColumnsThisTable=" + nbrColumnsThisTable);
      if (nbrColumnsThisTable <= 0) {
        throw new TestException("Expected " + BackupAndRestorePrms.class.getName() + ".nbrColumnsPerTable to be > 0");
      }
      tableDef = new TableDefinition(SCHEMA_NAME, tableName);

      // Determine which tables have lobs
      nbrLobColumnsThisTable = 0;
      if (nbrLobColumns > 0) {
        nbrLobColumnsThisTable = nbrLobColumns / nbrTables;
        if (nbrLobColumns % nbrTables > 0) {
          nbrLobColumnsThisTable++;
        }
        nbrLobColumns--;
      }
      // Build the table definition, the 1st column being the primary key
      tableDef.addColumn("pKey", DATA_TYPE_VARCHAR, "20", true);
      // Now setup the rest of the columns for the table
      int startingColNbr = 2;
      for (int columnNbr = startingColNbr;columnNbr <= nbrColumnsThisTable;columnNbr++) {
        String columnName = "col_" + columnNbr;
        String dataType = BackupAndRestorePrms.getColumnType();
        if (nbrLobColumnsThisTable > 0) {
          dataType = BackupAndRestorePrms.getLobColumnType();
          nbrLobColumnsThisTable--;
        }
        if (dataType.equalsIgnoreCase(DATA_TYPE_VARCHAR)) {
          tableDef.addColumn(columnName, dataType, "" + BackupAndRestorePrms.getVarCharLength(), false);
        } else if (dataType.equalsIgnoreCase(DATA_TYPE_CLOB)) {
          tableDef.addColumn(columnName, dataType, BackupAndRestorePrms.getLobLength(), false);
        } else {
          tableDef.addColumn(columnName, dataType, false);
        }
      }
      tableList.add(tableDef);
    }
    logWriter.info("BackupRestoreBigDataTest.generateTableDefinitions-Table Count=" + tableList.size());
    BackupAndRestoreBB.getBB().getSharedMap().put(KEY_TABLES, tableList);
  }

  /**
   * Hydra task to create Tables. This executes the create table sql generated by HydraTask_generateTableDefinitions().
   * Create tables already defined in the blackboard.
   */
  @SuppressWarnings("unchecked cast")
  public static void HydraTask_createTables() {
    List<TableDefinition> tableList = (List<TableDefinition>) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TABLES);
    String createStmt;
    for (TableDefinition table : tableList) {
      createStmt = table.getCreateTableStatement() + " " + BackupAndRestorePrms.getExtraTableClause();
      logWriter.info("BackupRestoreBigDataTest.createTables-About to execute sql statement: " + createStmt);
      executeSqlStatement(createStmt, true);
    }
    logWriter.info("BackupRestoreBigDataTest.createTables-Created " + tableList.size() + " tables");

    setEvictionHeapPercentage();

    Connection conn = (Connection) threadLocal_gfxdConnection.get();
    ResultSet rs =
      executeSqlQuery(conn, "SELECT tablename FROM sys.systables WHERE tabletype = 'T' AND tableschemaname = '" +
                            SCHEMA_NAME.toUpperCase() + "'");
    try {
      rs.beforeFirst();
      String tableName;
      availableTables = new TreeSet<String>();
      while (rs.next()) {
        tableName = rs.getString(1);
        availableTables.add(tableName);
      }
    } catch (SQLException e) {
      throw new TestException(TestHelper.getStackTrace(e));
    }
    StringBuilder aStr = new StringBuilder("Tables from " + SCHEMA_NAME + " schema:\n");
    if (availableTables.size() == 0) {
      aStr.append("   Table is empty");
    } else {
      for (String tableName : availableTables) {
        aStr.append("  ").append(tableName).append("\n");
      }
    }
    logWriter.info("BackupRestoreBigDataTest.HydraTask_createTables:" + aStr.toString());
  }

  /**
   * This is intended to change the default eviction percentage to the value specified in the
   * sql.backupAndRestore.BackupAndRestorePrms-evictionHeapPercentage parameter.
   */
  private static void setEvictionHeapPercentage() {
    try {
      Connection conn = (Connection) threadLocal_gfxdConnection.get();

      String getEvictionHeapPercentageSql = "values SYS.GET_EVICTION_HEAP_PERCENTAGE()";
      ResultSet resultSet = executeSqlQuery(conn, getEvictionHeapPercentageSql);
      resultSet.beforeFirst();
      while (resultSet.next()) {
        logWriter.info("BackupRestoreBigDataTest.setEvictionHeapPercentage-percentage before set=" +
                       resultSet.getDouble(1));
      }

      String setEvictionHeapPercentageSql = "CALL SYS.SET_EVICTION_HEAP_PERCENTAGE(?)";
      double evictionHeapPercentage = BackupAndRestorePrms.getEvictionHeapPercentage();
      logWriter.info("BackupRestoreBigDataTest.setEvictionHeapPercentage-sql=" + setEvictionHeapPercentageSql +
                     " with args " + evictionHeapPercentage);

      CallableStatement cs = conn.prepareCall(setEvictionHeapPercentageSql);
      cs.setDouble(1, evictionHeapPercentage);
      cs.executeUpdate();

      resultSet = executeSqlQuery(conn, getEvictionHeapPercentageSql);
      resultSet.beforeFirst();
      while (resultSet.next()) {
        logWriter.info("BackupRestoreBigDataTest.setEvictionHeapPercentage-percentage after set=" +
                       resultSet.getDouble(1));
      }

    } catch (SQLException se) {
      SQLHelper.handleSQLException(se);
    }
  }

  /**
   * Hydra task to create Indexes. Intentions are to pick a random column from each table to use as an Index. Code ensures
   * that the chosen column is not the PK or a Clob.
   */
  @SuppressWarnings("unchecked cast")
  public static void HydraTask_createIndexes() {
    GsRandom rand = TestConfig.tab().getRandGen();
    int nbrOfColumns;
    String createStmt, fullTableName;
    List<TableDefinition> tableList = (List<TableDefinition>) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TABLES);
    for (TableDefinition table : tableList) {
      fullTableName = table.getFullTableName();
      nbrOfColumns = table.getNumColumns();
      // Get a Random Column to use as the Index. Don't get the first column (PK), or a CLOB column
      // #  0  1    2    3    4
      // 5  pk clob date vc   int
      // 5  pk clob clob clob clob
      int startingColNbr = 0;
      boolean validColumnType = false;
      do {
        startingColNbr++;
        // If we've gone through all of the columns then leave
        if (startingColNbr == nbrOfColumns) {
          break;
        }
        // Make sure that the column type is not CLOB
        validColumnType = !table.getColumnType(startingColNbr).equalsIgnoreCase(DATA_TYPE_CLOB);
      } while (!validColumnType);

      if (validColumnType) {
        int randColNbr = rand.nextInt(startingColNbr, nbrOfColumns - 1);
        String randColName = table.getColumnName(randColNbr);
        String indexName = "idx" + fullTableName.substring(fullTableName.length() - 3, fullTableName.length()) +
                           "_" + (randColNbr + 1);
        // Check the 1st bit of the randColNbr, if it is on, then let's use Ascending Order otherwise use Descending
        String order = (randColNbr & 1) == 1 ? "ASC" : "DESC";
        createStmt = "CREATE INDEX " + indexName +
                     " ON " + fullTableName +
                     " (" + randColName + " " + order + ")";
        logWriter.info("BackupRestoreBigDataTest.HydraTask_createIndexes-About to execute sql statement: " + createStmt);
        executeSqlStatement(createStmt, true);
      } else {
        logWriter.info("BackupRestoreBigDataTest.HydraTask_createIndexes-An index could not be created for table '" +
                       fullTableName + "' because all columns are CLOBS.");
      }
    }
  }

  /**
   * Execute a sql statement using jdbc with this thread's connection
   *
   * @param sqlStatement The string containing the sql statement to execute.
   * @param logStatement If true, then log the statement, if false do not log.
   */
  private static void executeSqlStatement(String sqlStatement, boolean logStatement) {
    Connection conn = (Connection) threadLocal_gfxdConnection.get();
    try {
      Statement stmt = conn.createStatement();
      try {
        if (logStatement) {
          logWriter.info("BackupRestoreBigDataTest.executeSqlStatement-Executing sql statement: '" + sqlStatement + "'");
        }
        long startTime = System.currentTimeMillis();
        stmt.execute(sqlStatement);
        long duration = System.currentTimeMillis() - startTime;
        if (logStatement) {
          logWriter.info("BackupRestoreBigDataTest.executeSqlStatement-Done executing sql statement: '" + sqlStatement +
                         "' in " + duration + "ms");
        }
      } finally {
        if (stmt != null) {
          stmt.close();
        }
      }
    } catch (SQLException e) {
      throw new TestException(TestHelper.getStackTrace(e));
    }
  }

  /**
   * Execute an sql query and return the result set.
   *
   * @param sqlQuery The query string to execute.
   *
   * @return The result set for the query.
   */
  private static ResultSet executeSqlQuery(Connection conn, String sqlQuery) {
    try {
      Statement stmt = conn.createStatement(ResultSet.TYPE_SCROLL_INSENSITIVE, ResultSet.CONCUR_READ_ONLY);
      logWriter.info("BackupRestoreBigDataTest.executeSqlQuery-Executing sql query: " + sqlQuery);
      long startTime = System.currentTimeMillis();
      ResultSet rs = stmt.executeQuery(sqlQuery);
      long duration = System.currentTimeMillis() - startTime;
      logWriter.info("BackupRestoreBigDataTest.executeSqlQuery-Done executing sql query: " + sqlQuery +
                     " in " + duration + "ms");
      return rs;
    } catch (SQLException e) {
      throw new TestException(TestHelper.getStackTrace(e));
    }
  }

  /**
   * Initialize prepared statements for inserts, updates & deletes (in the thin clients)
   */
  @SuppressWarnings("unchecked cast")
  public static void HydraTask_createPreparedStatements() {
    List<TableDefinition> tableList = (List<TableDefinition>) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TABLES);
    if (tableList == null) {
      throw new TestException("Test problem, no tableList defined, this task should execute after HydraTask_generateTableDefinitions");
    }
    Connection conn = (Connection) threadLocal_gfxdConnection.get();
    threadLocal_insertPreparedStatements.set(createPreparedInsertStmtMap(conn, tableList));
    threadLocal_updatePreparedStatements.set(createPreparedUpdateStmtMap(conn, tableList));
    threadLocal_deletePreparedStatements.set(createPreparedDeleteStmtMap(conn, tableList));
  }

  /**
   * Create and return a Map containing insert statements and prepared statements per table.
   *
   * @param tableList A List of TableDefinitions, one per table
   *
   * @return A Map key: (String) fully qualified table name value: (List), at index 0: String insert statement for the
   * table at index 1: A prepared statement for the table
   */
  private static Map<String, List> createPreparedInsertStmtMap(Connection conn, List<TableDefinition> tableList) {
    Map<String, List> insertStmtMap = new HashMap<String, List>();
    for (TableDefinition table : tableList) {
      // Create the insert statement, suitable for a prepared statement
      StringBuilder insertStmt = new StringBuilder();
      String fullTableName = table.getFullTableName();
      insertStmt.append("INSERT INTO ").append(fullTableName).append(" (");
      int nbrColumns = table.getNumColumns();
      for (int colNbr = 1;colNbr <= nbrColumns;colNbr++) {
        insertStmt.append(table.getColumnName(colNbr - 1));
        if (colNbr != nbrColumns) {
          insertStmt.append(", ");
        }
      }
      insertStmt.append(") VALUES (");
      for (int colNbr = 1;colNbr <= nbrColumns;colNbr++) {
        insertStmt.append("?");
        if (colNbr != nbrColumns) {
          insertStmt.append(", ");
        }
      } // done iterating all columns for this table
      insertStmt.append(")");

      // create the prepared statement
      CallableStatement preparedStmt;
      try {
        preparedStmt = conn.prepareCall(insertStmt.toString());
      } catch (SQLException e) {
        throw new TestException(TestHelper.getStackTrace(e));
      }

      // add to the map
      List<Object> stmtList = new ArrayList<Object>();
      stmtList.add(insertStmt.toString());
      logWriter.info("insert statement is: " + insertStmt.toString());
      stmtList.add(preparedStmt);
      insertStmtMap.put(fullTableName, stmtList);
    }
    logWriter.fine("BackupRestoreBigDataTest.createPreparedInsertStmtMap-Created prepared statement map: " + insertStmtMap);
    return insertStmtMap;
  }

  /**
   * Create and return a Map containing insert statements and prepared statements per table.
   *
   * @param tableList A List of TableDefinitions, one per table
   *
   * @return A Map key: (String) fully qualified table name value: (List), at index 0: String insert statement for the
   * table at index 1: A prepared statement for the table
   */
  private static Map<String, List> createPreparedUpdateStmtMap(Connection conn, List<TableDefinition> tableList) {
    Map<String, List> updateStmtMap = new HashMap<String, List>();
    for (TableDefinition table : tableList) {
      // Create the update statement, suitable for a prepared statement
      StringBuilder updateStmt = new StringBuilder();
      String fullTableName = table.getFullTableName();
      updateStmt.append("UPDATE ").append(fullTableName).append(" SET ");
      int nbrColumns = table.getNumColumns();
      // table is partitioned by the 1st column (pKey) skipping this column because:
      // Feature not implemented: Update of partitioning column not supported.
      //0, 1, 2 = 3 cols
      for (int colNbr = 1;colNbr < nbrColumns;colNbr++) {
        updateStmt.append(table.getColumnName(colNbr)).append("=?");
        if (colNbr != (nbrColumns - 1)) {
          updateStmt.append(", ");
        }
      }
      updateStmt.append(" WHERE pKey=?");

      // create the prepared statement
      CallableStatement preparedStmt;
      try {
        preparedStmt = conn.prepareCall(updateStmt.toString());
      } catch (SQLException e) {
        throw new TestException(TestHelper.getStackTrace(e));
      }

      // add to the map
      List<Object> stmtList = new ArrayList<Object>();
      stmtList.add(updateStmt.toString());
      logWriter.info("update statement is: " + updateStmt.toString());
      stmtList.add(preparedStmt);
      updateStmtMap.put(fullTableName, stmtList);
    }
    logWriter
      .fine("BackupRestoreBigDataTest.createPreparedUpdateStmtMap-Created prepared statement map: " + updateStmtMap);
    return updateStmtMap;
  }

  /**
   * Create and return a Map containing insert statements and prepared statements per table.
   *
   * @param tableList A List of TableDefinitions, one per table
   *
   * @return A Map key: (String) fully qualified table name value: (List), at index 0: String insert statement for the
   * table at index 1: A prepared statement for the table
   */
  private static Map<String, List> createPreparedDeleteStmtMap(Connection conn, List<TableDefinition> tableList) {
    Map<String, List> deleteStmtMap = new HashMap<String, List>();
    for (TableDefinition table : tableList) {
      // Create the delete statement, suitable for a prepared statement
      StringBuilder deleteStmt = new StringBuilder();
      String fullTableName = table.getFullTableName();
      deleteStmt.append("DELETE FROM ").append(fullTableName).append(" WHERE pKey=?");

      // create the prepared statement
      CallableStatement preparedStmt;
      try {
        preparedStmt = conn.prepareCall(deleteStmt.toString());
      } catch (SQLException e) {
        throw new TestException(TestHelper.getStackTrace(e));
      }

      // add to the map
      List<Object> stmtList = new ArrayList<Object>();
      stmtList.add(deleteStmt.toString());
      logWriter.info("delete statement is: " + deleteStmt.toString());
      stmtList.add(preparedStmt);
      deleteStmtMap.put(fullTableName, stmtList);
    }
    logWriter
      .fine("BackupRestoreBigDataTest.createPreparedDeleteStmtMap-Created prepared statement map: " + deleteStmtMap);
    return deleteStmtMap;
  }

  /**
   *
   */
  public static void HydraTask_clickStopWatch() {
    // Store the time we start the data initialization
    String keySuffix = BackupAndRestorePrms.getTimingKeySuffix();
    BackupAndRestoreBB.getBB().getSharedMap().put(KEY_TIMINGS + keySuffix, System.nanoTime());
  }

  /**
   * Use this from the Client Threads to populate some data prior to performing the initial full backup.
   * How much data is determined by the sql.backupAndRestore.BackupAndRestorePrms-initialDataMB parameter.
   */
  public static void HydraTask_doDataInitialization() {
    SharedCounters sharedCounters = BackupAndRestoreBB.getBB().getSharedCounters();
    // Determine if this thread is the leader, if so this thread is leader for the duration of the test
    Object value = threadLocal_isClientLeader.get();
    boolean isClientLeader;
    if (value == null) { // 1st time through
      isClientLeader = (sharedCounters.incrementAndRead(BackupAndRestoreBB.theClientLeader)) == 1;
      threadLocal_isClientLeader.set(isClientLeader);
    } else { // we've been here before, get the value
      isClientLeader = (Boolean) value;
    }
    logWriter.fine("BackupRestoreBigDataTest.HydraTask_doDataInitialization-isClientLeader=" + isClientLeader);

    Connection conn = (Connection) threadLocal_gfxdConnection.get();
    boolean reusePreparedStatements = BackupAndRestorePrms.getReusePreparedStatements();
    logWriter.fine("BackupRestoreBigDataTest.HydraTask_doDataInitialization-reusePreparedStatements=" + reusePreparedStatements);

    // Insert some data
    long msToRun = (TestConfig.tab().longAt(TestHelperPrms.minTaskGranularitySec, 30) * TestHelper.SEC_MILLI_FACTOR);
    long startTime = System.currentTimeMillis();
    do {
      int pkIndex = new Long(sharedCounters.incrementAndRead(BackupAndRestoreBB.largestKey)).intValue();
      logWriter.fine("BackupRestoreBigDataTest.HydraTask_doDataInitialization-pkIndex=" + pkIndex);
      insertRowIntoAllTables(conn, pkIndex, reusePreparedStatements);
    } while (((System.currentTimeMillis() - startTime) < msToRun) && !hasInitialDataSizeBeenMet());

    if (hasInitialDataSizeBeenMet()) {
      if (isClientLeader) {
        double mbInUse = sharedCounters.read(BackupAndRestoreBB.totalDataBytes) / MEGABYTE;
        int largestPK = new Long(sharedCounters.read(BackupAndRestoreBB.largestKey)).intValue();
        logWriter.info("BackupRestoreBigDataTest.HydraTask_doDataInitialization-The initial data load is finished. Created " +
                       mbInUse + " MB of data in " + largestPK + " total records.");
      }
      throw new StopSchedulingTaskOnClientOrder("The initial data load is finished. Stopping this client's InitTask.");
    }
  }

  /**
   * Use this to determine if the desired (sql.backupAndRestore.BackupAndRestorePrms-initialDataMB) amount of initialized
   * data has been met.
   *
   * @return true if the generated data size has been met
   */
  private static boolean hasInitialDataSizeBeenMet() {
    double mbInUse = BackupAndRestoreBB.getBB().getSharedCounters().read(BackupAndRestoreBB.totalDataBytes) / MEGABYTE;
    return mbInUse >= BackupAndRestorePrms.getInitialDataMB();
  }

  /**
   * Given a primary key index, insert a row with that primary key into all tables
   *
   * @param conn The Connection to use to connect to the data
   * @param pkIndex The index of the primary key to insert.
   */
  @SuppressWarnings("unchecked cast")
  private static void insertRowIntoAllTables(Connection conn, int pkIndex, boolean reusePreparedStatements) {
    CallableStatement preparedStmt;
    boolean closePreparedStatement;
    long totalRowLength;
    Map<String, List> insertStmtMap = (Map<String, List>) threadLocal_insertPreparedStatements.get();
    List stmtList;
    List<Object> preparedStmtArgs;
    String insertStmt;

    List<TableDefinition> tableList = (List<TableDefinition>) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TABLES);
    for (TableDefinition table : tableList) {
      stmtList = insertStmtMap.get(table.getFullTableName());
      insertStmt = (String) stmtList.get(0);
      closePreparedStatement = false;
      if (reusePreparedStatements) { // reuse prepared statement from the map
        preparedStmt = (CallableStatement) stmtList.get(1);
      } else { // get a new prepared statement each insert
        try {
          preparedStmt = conn.prepareCall(insertStmt);
        } catch (SQLException e) {
          throw new TestException(TestHelper.getStackTrace(e));
        }
        closePreparedStatement = true; // close the prepared statement if creating a new one each insert
      }

      // create a list of arguments for the prepared statement
      preparedStmtArgs = buildInsertPreparedStmtArgs(conn, table, pkIndex);

      // execute the prepared statement
      int result = executePreparedStatementUpdate(preparedStmt, insertStmt, preparedStmtArgs);
      totalRowLength = calculateRowLength(preparedStmtArgs);
      if (result > 0) {
        BackupAndRestoreBB.getBB().getSharedCounters().add(BackupAndRestoreBB.totalDataBytes, totalRowLength);
      } else {
        throw new TestException("An Error has occurred while Inserting a row into the " + table.getFullTableName() +
                                " table. result=" + result +
                                ", insertStmt=" + insertStmt +
                                ", preparedStmtArgs=" + reportPreparedStmtArgs(preparedStmtArgs) +
                                ".");
      }
      
     
      if (closePreparedStatement) {
        try {
          preparedStmt.close();
        } catch (SQLException e) {
          throw new TestException(TestHelper.getStackTrace(e));
        }
      }
      // release the Clob objects
      for (Object stmtArg : preparedStmtArgs) {
        if (stmtArg instanceof Clob) {
          try {
            ((Clob) stmtArg).free();
          } catch (SQLException e) {
            throw new TestException(TestHelper.getStackTrace(e));
          }
        }
      }
      
      if (setTx) {
        try {
          conn.commit();
        } catch (SQLException se) {
          SQLHelper.handleSQLException(se);
        }
      }
    }
  }

  /**
   * This is a required setup task that needs to be called from each Client Thread to ensure that the OpTrackers
   * are instantiated and saved for later use.
   */
  public static void HydraTask_setupOpTrackers() {
    // Create an OpTracker for this Thread
    int maxStartKey = new Long(BackupAndRestoreBB.getBB().getSharedCounters().read(BackupAndRestoreBB.largestKey)).intValue();
    int nbrOfClientThreads = TestConfig.getInstance().getThreadGroup(THREADGROUP_CLIENT).getTotalThreads();
    nbrOfClientThreads += TestConfig.getInstance().getThreadGroup(THREADGROUP_DDL).getTotalThreads();
    OpTracker opTracker = new OpTracker(maxStartKey, nbrOfClientThreads);
    // Store it to the ThreadLocal
    threadLocal_opTracker.set(opTracker);
  }

  /**
   * Client Threads use this to perform random operations. Terminating of this Task is handled by the Server Thread
   * that is executing HydraTask_doBackups
   */
  public static void HydraTask_doRandomOps() {
    SharedCounters sharedCounters = BackupAndRestoreBB.getBB().getSharedCounters();
    if (sharedCounters.read(BackupAndRestoreBB.PauseOps) > 0) {
      logWriter.info("BackupRestoreBigDataTest.HydraTask_doRandomOps-It is time to pause client ops");
      if ((Boolean) threadLocal_clientThreadIsPaused.get()) {
        logWriter.info("BackupRestoreBigDataTest.HydraTask_doRandomOps-This client thread is already paused");
      } else {
        logWriter.info("BackupRestoreBigDataTest.HydraTask_doRandomOps-This client thread is now paused");
        threadLocal_clientThreadIsPaused.set(true);
        sharedCounters.increment(BackupAndRestoreBB.PauseOps);
      }
      MasterController.sleepForMs(5000);
    } else {
      // Not Pausing - Do Ops
      threadLocal_clientThreadIsPaused.set(false);
      clientRandomOps();
    }
  }

  /**
   * This is the main Task to be called by the Client Threads to perform random operations.
   */
  private static void clientRandomOps() {
    SharedCounters sharedCounters = BackupAndRestoreBB.getBB().getSharedCounters();

    Connection conn = (Connection) threadLocal_gfxdConnection.get();

    // Make sure we have a proper OpTracker for this Thread
    OpTracker opTracker = (OpTracker) threadLocal_opTracker.get();

    boolean reusePreparedStatements = BackupAndRestorePrms.getReusePreparedStatements();
    logWriter.fine("BackupRestoreBigDataTest.clientsDoOps-This thread is doing ops, reusePreparedStatements=" +
                   reusePreparedStatements);

    // Calculate the number of rows to insert / update / delete (based on the insertPercent & the updatePercent)
    // We divide the insertPercent by two because each thread will perform inserts but only 1/2 the threads will do
    // updates and the other 1/2 will do deletes
    int insertPercent = BackupAndRestorePrms.getInsertPercent() / 2;
    int updatePercent = BackupAndRestorePrms.getUpdatePercent();
    int deletePercent = BackupAndRestorePrms.getDeletePercent();
    GsRandom rand = TestConfig.tab().getRandGen();
    int opType, pkIndex;

    long msToRun = (TestConfig.tab().longAt(TestHelperPrms.minTaskGranularitySec, 30) * TestHelper.SEC_MILLI_FACTOR);
    long startTime = System.currentTimeMillis();
    do {
      if (opTracker.getOpType() == OpTracker.OP_TYPE_UPDATE) {
        // If this thread has an odd-numbered thread id then we are just doing inserts and updates
        // Ex. insertPercent = 80, updatePercent = 10, deletePercent = 10 | Total = 100%
        //  for updates: insertPercent = (80/2) = 40, updatePercent = 10  | Total = 50% (the other 50% is for deletes)
        //  now we get a random number between 1 and 50. if this number is less than or equal to 40 we have an insert
        //  otherwise an update
        if (rand.nextInt(1, (insertPercent + updatePercent)) <= insertPercent) {
          opType = OpTracker.OP_TYPE_INSERT;
        } else {
          opType = OpTracker.OP_TYPE_UPDATE;
        }
      } else {
        // If this thread has an even-numbered thread id then we are just doing inserts and deletes
        // Ex. insertPercent = 80, updatePercent = 10, deletePercent = 10 | Total = 100%
        //  for updates: insertPercent = (80/2) = 40, deletePercent = 10  | Total = 50% (the other 50% is for updates)
        //  now we get a random number between 1 and 50. if this number is less than or equal to 40 we have an insert
        //  otherwise an delete
        if (rand.nextInt(1, (insertPercent + deletePercent)) <= insertPercent) {
          opType = OpTracker.OP_TYPE_INSERT;
        } else {
          opType = OpTracker.OP_TYPE_DELETE;
        }
      }

      // Get the Primary Key to be used in the Op
      if (opType == OpTracker.OP_TYPE_INSERT) {
        // When Inserting new data, increment the PK Counter
        pkIndex = new Long(sharedCounters.incrementAndRead(BackupAndRestoreBB.largestKey)).intValue();
      } else {
        // When Updating or Deleting existing data, get the PK from the OpTracker
        // But first, we need to rebase the OpTracker to use the latest, largest Primary Key
        opTracker.rebaseAt(new Long(sharedCounters.read(BackupAndRestoreBB.largestKey)).intValue());

        pkIndex = opTracker.nextKeyForOp();
        if (pkIndex == -1) {
          logWriter.error("BackupRestoreBigDataTest.clientsDoOps-pkIndex=" + pkIndex +
                          " is not ready in the OpTracker, skipping this update / delete");
        }
        logWriter.fine("BackupRestoreBigDataTest.clientsDoOps-pkIndex=" + pkIndex +
                       ", opType=" + opType +
                       ", opTracker=" + opTracker.toString());
      }
      performAnOpOnAllTables(conn, pkIndex, reusePreparedStatements, opType);
    } while (((System.currentTimeMillis() - startTime) < msToRun) && sharedCounters.read(BackupAndRestoreBB.PauseOps) == 0);
    threadLocal_opTracker.set(opTracker);
  }

  /**
   * Given a primary key index, insert a row with that primary key into all tables
   *
   * @param conn The Connection to use to connect to the data
   * @param pkIndex The index of the primary key to insert.
   */
  @SuppressWarnings("unchecked cast")
  private static void performAnOpOnAllTables(Connection conn,
                                             int pkIndex,
                                             boolean reusePreparedStatements,
                                             int opType) {
    CallableStatement preparedStmt;
    boolean closePreparedStatement;
    String sqlStmt;
    List<Object> preparedStmtArgs;
    List stmtList;

    Map<String, List> preparedStmtMap;
    if (opType == OpTracker.OP_TYPE_INSERT) {
      preparedStmtMap = (Map<String, List>) threadLocal_insertPreparedStatements.get();
    } else if (opType == OpTracker.OP_TYPE_UPDATE) {
      preparedStmtMap = (Map<String, List>) threadLocal_updatePreparedStatements.get();
    } else {
      preparedStmtMap = (Map<String, List>) threadLocal_deletePreparedStatements.get();
    }

    List<TableDefinition> tableList = (List<TableDefinition>) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TABLES);
    for (TableDefinition table : tableList) {
      stmtList = preparedStmtMap.get(table.getFullTableName());
      sqlStmt = (String) stmtList.get(0);
      closePreparedStatement = false;
      if (reusePreparedStatements) { // reuse prepared statement from the map
        preparedStmt = (CallableStatement) stmtList.get(1);
      } else { // get a new prepared statement each insert
        try {
          preparedStmt = conn.prepareCall(sqlStmt);
        } catch (SQLException e) {
          throw new TestException(TestHelper.getStackTrace(e));
        }
        closePreparedStatement = true; // close the prepared statement if creating a new one each insert
      }

      // create a list of arguments for the prepared statement
      if (opType == OpTracker.OP_TYPE_INSERT) {
        preparedStmtArgs = buildInsertPreparedStmtArgs(conn, table, pkIndex);
      } else if (opType == OpTracker.OP_TYPE_UPDATE) {
        preparedStmtArgs = buildUpdatePreparedStmtArgs(conn, table, pkIndex);
      } else {
        preparedStmtArgs = buildDeletePreparedStmtArgs(table.getColumnType(0), pkIndex);
      }

      // execute the prepared statement
      int result = executePreparedStatementUpdate(preparedStmt, sqlStmt, preparedStmtArgs);
      
      if (result > 0) {
        if (opType == OpTracker.OP_TYPE_INSERT) {
          long totalRowLength = calculateRowLength(preparedStmtArgs);
          logWriter.fine("BackupRestoreBigDataTest.performAnOpOnAllTables-totalRowLength=" + totalRowLength);
          BackupAndRestoreBB.getBB().getSharedCounters().add(BackupAndRestoreBB.totalDataBytes, totalRowLength);
        } else if (opType == OpTracker.OP_TYPE_UPDATE) {
          BackupAndRestoreBB.getBB().getSharedCounters().increment(BackupAndRestoreBB.nbrOfUpdates);
        } else if (opType == OpTracker.OP_TYPE_DELETE) {
          BackupAndRestoreBB.getBB().getSharedCounters().increment(BackupAndRestoreBB.nbrOfDeletes);
        }
      } else {
        throw new TestException("An Error has occurred while Inserting a row into the " + table.getFullTableName() +
                                " table. result=" + result +
                                ", opType=" + opType +
                                ", sqlStmt=" + sqlStmt +
                                ", preparedStmtArgs=" + reportPreparedStmtArgs(preparedStmtArgs) +
                                ".");
      }

      if (closePreparedStatement) {
        try {
          preparedStmt.close();
        } catch (SQLException e) {
          throw new TestException(TestHelper.getStackTrace(e));
        }
      }
      // release the Clob objects
      for (Object stmtArg : preparedStmtArgs) {
        if (stmtArg instanceof Clob) {
          try {
            ((Clob) stmtArg).free();
          } catch (SQLException e) {
            throw new TestException(TestHelper.getStackTrace(e));
          }
        }
      }
       
      if (setTx) {
        try {
          conn.commit();
        } catch (SQLException se) {
          SQLHelper.handleSQLException(se);
        }
      }
    }
  }

  /**
   * Use this to generate the List of Objects to be used as arguments in PreparedStatement (executeUpdate) to perform
   * an Sql Insert.
   *
   * @param conn The thin client driver Connection to GemFireXD
   * @param table The TableDefinition for the table to be used to determine the column type that used in the statement's
   *              values clause
   * @param pkIndex an int that represents the argument to be used as the primary key
   *
   * @return A List of Objects intended to be used as arguments in a PreparedStatement (executeUpdate)
   */
  private static List<Object> buildInsertPreparedStmtArgs(Connection conn, TableDefinition table, int pkIndex) {
    //INSERT INTO bigDataTest.Table_003 (pKey, col_2) VALUES (?,?)
    List<Object> preparedStmtArgs = new ArrayList<Object>();
    String dataType;

    int nbrColumns = table.getNumColumns();
    for (int colNbr = 0;colNbr < nbrColumns;colNbr++) {
      dataType = table.getColumnType(colNbr);
      if (DATA_TYPE_INTEGER.equalsIgnoreCase(dataType)) {
        preparedStmtArgs.add(pkIndex);

      } else if (DATA_TYPE_VARCHAR.equalsIgnoreCase(dataType)) {
        if (colNbr == 0) { // primary key
          preparedStmtArgs.add(PREFIX_PK + pkIndex);
        } else {
          preparedStmtArgs.add(buildRandomString(Integer.valueOf(table.getColumnLength(colNbr)), PREFIX_INSERT));
        }

      } else if (DATA_TYPE_DATE.equalsIgnoreCase(dataType)) {
        preparedStmtArgs.add(insertDate);

      } else if (DATA_TYPE_CLOB.equalsIgnoreCase(dataType)) {
        preparedStmtArgs.add(buildRandomClobObject(conn, table.getColumnLength(colNbr), PREFIX_INSERT));

      } else {
        throw new TestException("BackupRestoreBigDataTest.buildInsertPreparedStmtArgs-Test does not currently support dataType " + dataType);
      }
    } // done iterating all columns for this table

    return preparedStmtArgs;
  }

  /**
   * Use this to generate the List of Objects to be used as arguments in PreparedStatement (executeUpdate) to perform
   * an Sql Update.
   *
   * @param conn The thin client driver Connection to GemFireXD
   * @param table The TableDefinition for the table to be used to determine the column type that used in the statement's
   *              where clause
   * @param pkIndex an int that represents the argument to be used in the where clause
   *
   * @return A List of Objects intended to be used as arguments in a PreparedStatement (executeUpdate)
   */
  private static List<Object> buildUpdatePreparedStmtArgs(Connection conn, TableDefinition table, int pkIndex) {
    //UPDATE bigDataTest.Table_003 SET col_2=?, col_3=? WHERE pKey=?
    List<Object> preparedStmtArgs = new ArrayList<Object>();
    String dataType;

    // table is partitioned by the 1st column (pKey) skipping this column because:
    // Feature not implemented: Update of partitioning column not supported.
    for (int colNbr = 1;colNbr < table.getNumColumns();colNbr++) {
      dataType = table.getColumnType(colNbr);
      if (DATA_TYPE_INTEGER.equalsIgnoreCase(dataType)) {
        preparedStmtArgs.add(pkIndex + 5);

      } else if (DATA_TYPE_VARCHAR.equalsIgnoreCase(dataType)) {
        preparedStmtArgs.add(buildRandomString(Integer.valueOf(table.getColumnLength(colNbr)), PREFIX_UPDATE));

      } else if (DATA_TYPE_DATE.equalsIgnoreCase(dataType)) {
        preparedStmtArgs.add(updateDate);

      } else if (DATA_TYPE_CLOB.equalsIgnoreCase(dataType)) {
        preparedStmtArgs.add(buildRandomClobObject(conn, table.getColumnLength(colNbr), PREFIX_UPDATE));

      } else {
        throw new TestException("BackupRestoreBigDataTest.buildUpdatePreparedStmtArgs-Test does not currently support dataType " + dataType);
      }
    } // done iterating all columns for this table now add the value for the where clause
    if (table.getColumnType(0).equalsIgnoreCase(DATA_TYPE_VARCHAR)) {
      preparedStmtArgs.add(PREFIX_PK + pkIndex);
    } else {
      preparedStmtArgs.add(pkIndex);
    }

    return preparedStmtArgs;
  }

  /**
   * Use this to generate a String object populated with random data.
   *
   * @param columnLength An int representing the length of the column.
   * @param dataPrefix A String representing what to prefix the String field's value with.
   *
   * @return The newly created String Object
   */
  private static String buildRandomString(int columnLength, String dataPrefix) {
    String dataValue = dataPrefix;
    GsRandom rand = TestConfig.tab().getRandGen();
    RandomValues rv = new RandomValues();
    RandomValues.setPrintableChars(true);

    int maxDataLen = columnLength - dataValue.length();
    int desiredDataLen = maxDataLen;
    if (rand.nextInt(1, 100) <= 80) { // random length sometimes
      desiredDataLen = rand.nextInt(1, maxDataLen);
    }
    dataValue += rv.getRandom_String('\'', desiredDataLen);

    return dataValue;
  }

  /**
   * Use this to generate a Clob object populated with random data.
   *
   * @param conn The thin client driver Connection to GemFireXD, used to create the clob.
   * @param columnLength A String representing the length of the column. Format: nK, nM, or nG.
   * @param dataPrefix A String representing what to prefix the Clob field's value with.
   *
   * @return The newly created Clob Object.
   */
  private static Clob buildRandomClobObject(Connection conn, String columnLength, String dataPrefix) {
    // Setup all the randomness
    long colLenBytes = getNbrBytes(columnLength);
    int firstThird = (int) (colLenBytes * 0.33);
    int secondThird = (int) (colLenBytes * 0.66);
    GsRandom rand = TestConfig.tab().getRandGen();
    long desiredDataLen;
    int randInt = rand.nextInt(1, 100);
    if (randInt <= 25) { // use a value from the first third
      desiredDataLen = rand.nextLong(1, firstThird);
    } else if (randInt <= 50) { // use a value from the second third
      desiredDataLen = rand.nextLong(firstThird + 1, secondThird);
    } else if (randInt <= 75) { // use a value from the last third
      desiredDataLen = rand.nextLong(secondThird + 1, colLenBytes);
    } else { // use the full size
      desiredDataLen = colLenBytes;
    }

    // Now fill the clob with random data
    Clob clobObj;
    try {
      RandomValues rv = new RandomValues();
      RandomValues.setPrintableChars(true);
      clobObj = conn.createClob();
      String clobValue = dataPrefix + rv.getRandom_String('\'', (desiredDataLen - dataPrefix.length()));
      clobObj.setString(1, clobValue);
      logWriter.fine("BackupRestoreBigDataTest.buildRandomClobObject-Created CLOB of size " + desiredDataLen +
                     " for a CLOB field of size " + columnLength);
    } catch (SQLException e) {
      throw new TestException(TestHelper.getStackTrace(e));
    }
    return clobObj;
  }

  /**
   * Use this to convert the prepared statement's arguments to a readable string
   *
   * @param preparedStmtArgs A List of Objects representing arguments to be passed to a PreparedStatement
   * @return String representing the arguments for a PreparedStatement as a readable String
   */
  private static String reportPreparedStmtArgs(List<Object> preparedStmtArgs) {
    String argsString = "";
    int argCntr = 0;

    for (Object smtArg : preparedStmtArgs) {
      if (argCntr > 0) {
        argsString += ",\n";
      }
      argsString += "Argument " + argCntr;
      if (smtArg instanceof String) {
        argsString += " (String) ='" + smtArg;
      } else if (smtArg instanceof Integer) {
        argsString += " (Integer) ='" + smtArg;
      } else if (smtArg instanceof Date) {
        argsString += " (Date) ='" + smtArg;
      } else if (smtArg instanceof Clob) {
        try {
          int length = PREFIX_UPDATE.length();
          argsString += " (Clob(1st " + length + " bytes)) ='"
                        + ((Clob) smtArg).getSubString(0, length);
        } catch (SQLException e) {
          throw new TestException(TestHelper.getStackTrace(e));
        }
      }
      argsString += "'";
      argCntr++;
    }

    return argsString;
  }

  /**
   * Given a String that is a byte specification in the form nK, nM or nG where n is an integer.
   *
   * @param byteSpec A byte specification.
   *
   * @return The total number of bytes specified in byteSpec.
   */
  private static long getNbrBytes(String byteSpec) {
    String intStr = "";
    int index = 0;
    while (index < byteSpec.length()) {
      char ch = byteSpec.charAt(index);
      if (Character.isDigit(ch)) {
        intStr = intStr + ch;
      } else {
        break;
      }
      index++;
    }
    int n = Integer.valueOf(intStr);
    String units = byteSpec.substring(index);
    if (units.equalsIgnoreCase("K")) {
      return n * 1024;
    } else if (units.equalsIgnoreCase("M")) {
      return n * MEGABYTE;
    } else if (units.equalsIgnoreCase("G")) {
      return n * GIGABYTE;
    } else {
      throw new TestException("Test problem: unknown byteSpec " + byteSpec);
    }
  }

  /**
   * Use this to generate the List of Objects to be used as arguments in PreparedStatement (executeUpdate) to perform
   * a Sql Delete
   *
   * @param columnType A String value for the type of column that will be use in the statement's where clause
   * @param pkIndex an int that represents the argument to be used in the where clause
   *
   * @return A List of Objects intended to be used as arguments in a PreparedStatement (executeUpdate)
   */
  private static List<Object> buildDeletePreparedStmtArgs(String columnType, int pkIndex) {
    List<Object> preparedStmtArgs = new ArrayList<Object>();

    //DELETE FROM bigDataTest.Table_003 WHERE pKey=?
    if (columnType.equalsIgnoreCase(DATA_TYPE_VARCHAR)) {
      preparedStmtArgs.add(PREFIX_PK + pkIndex);
    } else {
      preparedStmtArgs.add(pkIndex);
    }

    return preparedStmtArgs;
  }

  /**
   * Execute the given sql statement as a prepared statement with the given args.
   *
   * @param preparedStmt The CallableStatement to be executed
   * @param sqlStmt The sql statement text to execute.
   * @param args A List of Objects containing values to be used as the CallableStatement's arguments.
   *
   * @return The total bytes for the row that was inserted
   */
  private static int executePreparedStatementUpdate(CallableStatement preparedStmt, String sqlStmt, List<Object> args) {
    try {
      // Log the sql statement that would be executed by the Prepared Statement
      Object anArg;
      StringBuilder argsStr = new StringBuilder();
      for (int argCtr = 0;argCtr < args.size();argCtr++) {
        anArg = args.get(argCtr);
        if (anArg instanceof Clob) {
          argsStr.append("' a Clob of size ").append(((Clob) anArg).length()).append("'");
        } else {
          argsStr.append("'").append(anArg).append("'");
        }
        if (argCtr < (args.size() - 1)) {
          argsStr.append(", ");
        }
      }
      String pkIndex;
      if (sqlStmt.startsWith("INSERT")) {
        pkIndex = args.get(0).toString();
      } else {
        pkIndex = args.get(args.size() - 1).toString();
      }

      // Set the values for the Prepared Statement
      Object arg;
      for (int argIndex = 1;argIndex <= args.size();argIndex++) {
        arg = args.get(argIndex - 1);
        if (arg instanceof String) {
          preparedStmt.setString(argIndex, (String) arg);
        } else if (arg instanceof Integer) {
          preparedStmt.setInt(argIndex, (Integer) arg);
        } else if (arg instanceof Date) {
          preparedStmt.setDate(argIndex, (Date) arg);
        } else if (arg instanceof Clob) {
          preparedStmt.setClob(argIndex, (Clob) arg);
        }
      }

      // Execute the Prepared Statement
      logWriter.fine("BackupRestoreBigDataTest.executePreparedStatementUpdate-sqlStmt=" + sqlStmt +
                     " with arg(s): " + argsStr +
                     "), primaryKey is " + pkIndex);
      logWriter.info("Executing sqlStmt " + sqlStmt +
          " on primaryKey: " + pkIndex + ".");
      int result = preparedStmt.executeUpdate();
      logWriter.info("BackupRestoreBigDataTest.executePreparedStatementUpdate-result=" + result);
      return result;
    } catch (SQLException e) {
      throw new TestException(TestHelper.getStackTrace(e));
    }
  }

  /**
   * Use this along with the same arguments that would be passed to an Sql Insert statement to calculate the data length
   * of the row.
   *
   * @param args A List of Objects containing values that would be used as the Insert statement's value arguments.
   *
   * @return A long representing the number of bytes a table row would contain.
   */
  private static long calculateRowLength(List<Object> args) {
    long totalRowLength = 0;

    for (Object arg : args) {
      if (arg instanceof String) {
        totalRowLength += ((String) arg).length();
      } else if (arg instanceof Integer) {
        totalRowLength += 4;
      } else if (arg instanceof Date) {
        totalRowLength += 32; // All documentation I found says that a Date field is 32 bytes
      } else if (arg instanceof Clob) {
        try {
          totalRowLength += ((Clob) arg).length();
        } catch (SQLException e) {
          throw new TestException(TestHelper.getStackTrace(e));
        }
      }
    }

    return totalRowLength;
  }

  /**
   * Use this from a Server Thread to monitor the test and perform periodic incremental backups.
   * Backups are done at intervals determined by the sql.backupAndRestore.BackupAndRestorePrms-backupAfterMBofData parameter.
   */
  public static void HydraTask_doBackups() {
    logWriter.fine("BackupRestoreBigDataTest.HydraTask_doBackups");

    SharedCounters sharedCounters = BackupAndRestoreBB.getBB().getSharedCounters();
    // The total number of working client threads in this test
    if (sharedCounters.read(BackupAndRestoreBB.nbrOfThreadsToPause) == 0) {
      int nbrOfClientThreads = TestConfig.getInstance().getThreadGroup(THREADGROUP_CLIENT).getTotalThreads();
      nbrOfClientThreads += TestConfig.getInstance().getThreadGroup(THREADGROUP_DDL).getTotalThreads();
      nbrOfClientThreads++;   //1 for this thread
      sharedCounters.add(BackupAndRestoreBB.nbrOfThreadsToPause, nbrOfClientThreads);
    }

    // Give the Client VMs some time to do OPs
    int secondsForThreadToSleep = 30;
    logWriter.info("BackupRestoreBigDataTest.HydraTask_doBackups-Thread is going to sleep for " +
                   secondsForThreadToSleep + " seconds to allow other vms to do random ops");
    MasterController.sleepForMs(secondsForThreadToSleep * (int) TestHelper.SEC_MILLI_FACTOR);

    // Back up everything while ops are still processing
    if (isItTimeToBackup()) {
      triggerBackup();
    }

    if (hasDataOpsSizeBeenMet()) {
      // All of the Data has been created, Signal all clientThreads to pause
      sharedCounters.increment(BackupAndRestoreBB.PauseOps);

      // Wait for all Client Threads to Pause
      int totalThreadsToPause = (int) sharedCounters.read(BackupAndRestoreBB.nbrOfThreadsToPause);
      logWriter.info("BackupRestoreBigDataTest.HydraTask_doBackups-DataOps are done, " +
                     "lets pause all threads-totalThreadsToPause=" + totalThreadsToPause);
      TestHelper.waitForCounter(BackupAndRestoreBB.getBB(),
        "PauseOps",
        BackupAndRestoreBB.PauseOps,
        totalThreadsToPause,
        true,
        -1,
        2000);

      // Once all of the Clients have Paused, let's back up everything one last time
      triggerBackup();

      // Now stop the Task
      throw new StopSchedulingOrder("We are done generating data & performing backups.");
    }
  }

  /**
   * Use this to determine if the desired (sql.backupAndRestore.BackupAndRestorePrms-desiredDataOpsMB) amount of
   * initialized data has been met.
   *
   * @return true if the generated data size has been met
   */
  private static boolean hasDataOpsSizeBeenMet() {
    int initialDataMB = BackupAndRestorePrms.getInitialDataMB();
    int desiredDataOpsMB = BackupAndRestorePrms.getDesiredDataOpsMB();
    double mbInUse = BackupAndRestoreBB.getBB().getSharedCounters().read(BackupAndRestoreBB.totalDataBytes) / MEGABYTE;
    boolean sizeHasBeenMet = mbInUse >= initialDataMB + desiredDataOpsMB;
    logWriter.fine("BackupRestoreBigDataTest.hasDataOpsSizeBeenMet-initialDataMB=" + initialDataMB +
                   ", desiredDataOpsMB=" + desiredDataOpsMB +
                   ", mbInUse=" + mbInUse +
                   ", sizeHasBeenMet=" + sizeHasBeenMet);
    return sizeHasBeenMet;
  }

  /**
   * Use this to determine if it is time to perform an incremental backup. Determined by how much data has been
   * generated since the last backup and the sql.backupAndRestore.BackupAndRestorePrms-backupAfterMBofData parameter.
   *
   * @return true if it is time to perform a backup
   */
  private static boolean isItTimeToBackup() {
    int backupAfterMBofData = BackupAndRestorePrms.getBackupAfterMBofData();
    int initialDataMB = BackupAndRestorePrms.getInitialDataMB();
    SharedCounters sharedCounters = BackupAndRestoreBB.getBB().getSharedCounters();
    long backupCtr = sharedCounters.read(BackupAndRestoreBB.BackupCtr);
    double mbInUse = sharedCounters.read(BackupAndRestoreBB.totalDataBytes) / MEGABYTE;
    boolean timeToBackup = (mbInUse - initialDataMB) >= (backupAfterMBofData * (backupCtr + 1));
    logWriter.fine("BackupRestoreBigDataTest.isItTimeToBackup-backupAfterMBofData=" + backupAfterMBofData +
                   ", initialDataMB=" + initialDataMB +
                   ", backupCtr=" + backupCtr +
                   ", mbInUse=" + mbInUse +
                   ", timeToBackup=" + timeToBackup);
    return timeToBackup;
  }

  /**
   * Use this to call for a Full Backup
   */
  public static void HydraTask_doFullOnlineBackup() {
    triggerFullOnlineBackup();
  }

  /**
   * Use this to call for a backup. If this is the first backup a full backup will be performed. If a backup has already
   * been performed and the test is allowing incremental backups, an incremental backup will be performed.
   */
  private static void triggerBackup() {
    boolean doBackup = BackupAndRestorePrms.getDoBackup();
    logWriter.info("BackupRestoreBigDataTest.triggerBackup-doBackup=" + doBackup);
    if (doBackup) {
      // Check if we are to perform incremental backups
      boolean incrementalBackups = BackupAndRestorePrms.getIncrementalBackups();
      logWriter.info("BackupRestoreBigDataTest.triggerBackup-incrementalBackups=" + incrementalBackups);

      // How many backups have we done?
      long BackupCtr = BackupAndRestoreBB.getBB().getSharedCounters().read(BackupAndRestoreBB.BackupCtr);
      logWriter.info("BackupRestoreBigDataTest.triggerBackup-BackupCtr=" + BackupCtr);

      // If we are doing incremental backups and we have done at least one full backup, then we can do an incremental backup
      if (incrementalBackups && BackupCtr > 0) {
        triggerIncrementalOnlineBackup();
      } else {
        triggerFullOnlineBackup();
      }
    }
  }

  /**
   * Use this to call for a Full Backup
   */
  private static void triggerFullOnlineBackup() {
    performOnlineBackup(false);
  }

  /**
   * Use this to call for a Incremental Backup
   */
  private static void triggerIncrementalOnlineBackup() {
    performOnlineBackup(true);
  }

  /**
   * Use this to perform a system backup (incremental or full). The full GemFireXD command is built and pushed out the
   * file system for execution.
   *
   * @param incremental A boolean indicating whether this backup should be an incremental backup. 'true' = incremental,
   * 'false' = full.
   */
  private static void performOnlineBackup(boolean incremental) {
    logWriter.info("BackupRestoreBigDataTest.performOnlineBackup-incremental=" + incremental);

    // Get the backup storage location
    File rootBackupDir = new File(BackupAndRestorePrms.getBackupPath());
    logWriter.info("BackupRestoreBigDataTest.performOnlineBackup-rootBackupDir=" + rootBackupDir);

    long backupCtr = BackupAndRestoreBB.getBB().getSharedCounters().read(BackupAndRestoreBB.BackupCtr);
    logWriter.info("BackupRestoreBigDataTest.performOnlineBackup-backupCtr=" + backupCtr);

    // If this is to be an incremental backup, we need to build the 'baseline' parameter
    String incrementalClause = "";
    if (incremental) {
      String baselineDir = rootBackupDir + FILE_SEPARATOR + BACKUP_PREFIX + backupCtr;
      logWriter.info("BackupRestoreBigDataTest.performOnlineBackup-baselineDir1=" + baselineDir);

      // There should be one directory here, if not throw an error
      File[] baselineDirContents = new File(baselineDir).listFiles();
      if ((baselineDirContents != null ? baselineDirContents.length : 0) != 1) {
        throw new TestException("BackupRestoreBigDataTest.performOnlineBackup-On line backup cannot continue because we were " +
                                "expecting 1 subdirectory in the '" + baselineDir +
                                "' directory, but found " + (baselineDirContents != null ? baselineDirContents.length : 0));
      }
      // Get the name of the directory's one subdirectory and use this as the 'baseline' directory
      File backupDateDir = baselineDirContents[0];
      logWriter.info("BackupRestoreBigDataTest.performOnlineBackup-backupDateDir.getName()=" + backupDateDir.getName());
      baselineDir = baselineDir + FILE_SEPARATOR + backupDateDir.getName();
      logWriter.info("BackupRestoreBigDataTest.performOnlineBackup-baselineDir2=" + baselineDir);
      incrementalClause = BASELINE_OPT + baselineDir + " ";

    }
    logWriter.info("BackupRestoreBigDataTest.performOnlineBackup-incrementalClause=" + incrementalClause);

    // Get the path to SQLFire
    String gfxdCommand = FabricServerHelper.getGFXDCommand();
    Properties properties = DistributedSystemHelper.getDistributedSystem().getProperties();

    // Set the backup directory
    String backupDir = rootBackupDir + FILE_SEPARATOR + BACKUP_PREFIX + (backupCtr + 1);
    logWriter.info("BackupRestoreBigDataTest.performOnlineBackup-backupDir=" + backupDir);

    // Set the backup command
    String backupCmd = gfxdCommand +
                       " backup " + incrementalClause +
                       backupDir +
                       " -locators=" + properties.get("locators");

    logWriter.info("BackupRestoreBigDataTest.performOnlineBackup-backupCmd=" + backupCmd);
    String backupResult = null;
    try {
      backupResult = ProcessMgr.fgexec(backupCmd, 300);
    } catch (HydraRuntimeException e) {
      e.printStackTrace();
      logWriter.error("BackupRestoreBigDataTest.performOnlineBackup-Backup Failed: e.getCause()=" + e
        .getCause());
      logWriter.error("BackupRestoreBigDataTest.performOnlineBackup-Backup Failed: e.getMessage()=" + e
        .getMessage());
    }
    logWriter
      .info("BackupRestoreBigDataTest.performOnlineBackup-Done calling online backup tool, backupResult is " + backupResult);
    if (backupResult != null && !backupResult.contains("Backup successful")) {
      throw new TestException("BackupRestoreBigDataTest.performOnlineBackup-On line backup was not successful, backupResult from online backup is " + backupResult);
    }
    BackupAndRestoreBB.getBB().getSharedCounters().increment(BackupAndRestoreBB.BackupCtr);
  }

  /**
   * Use this after all data has been created to save the total number of rows in each table to the BB
   */
  public static void HydraTask_snapShotRowCounts() {
    Map<String, Number> tableRowCounts = getTableRowCounts();
    BackupAndRestoreBB.getBB().getSharedMap().put(KEY_TABLE_ROW_COUNTS, tableRowCounts);
  }

  /**
   * Use this after the restore has been executed to verify that the total number of rows have been restored for each
   * table
   */
  @SuppressWarnings("unchecked cast")
  public static void HydraTask_verifyRowCounts() {
    Map<String, Number> expRowCnts = (HashMap<String, Number>) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TABLE_ROW_COUNTS);
    Map<String, Number> actRowCnts = getTableRowCounts();

    StringBuilder errorSB = new StringBuilder();
    for (Map.Entry<String, Number> expEntry : expRowCnts.entrySet()) {
      Number expRowCnt = expEntry.getValue();
      Number actRowCnt = actRowCnts.get(expEntry.getKey());
      logWriter.info("BackupRestoreBigDataTest.HydraTask_verifyVerifiableData-For table '" + expEntry.getKey() +
                     "' we expected '" + expRowCnt + "' rows. Actual rows is '" + actRowCnt + "'.\n");
      if (expRowCnt.intValue() != actRowCnt.intValue()) {
        errorSB.append("Table ").append(expEntry.getKey()).append("'s row counts are incorrect. Expected ")
               .append(expRowCnt).append(" rows, but we have ").append(actRowCnt).append(" rows.");
      }
    }
    if (errorSB.length() != 0) {
      throw new TestException("Error! " + errorSB.toString());
    }
  }

  /**
   * Use this to calculate the number of rows in each of the tables
   *
   * @return A Map containing the table name (key) and the number of rows for that table (value)
   */
  private static Map<String, Number> getTableRowCounts() {
    Connection conn = (Connection) threadLocal_gfxdConnection.get();
    Map<String, Number> tableRowCounts = new HashMap<String, Number>();

    ResultSet rs;
    Number nbrOfRows;
    for (String tableName : availableTables) {
      rs = executeSqlQuery(conn, "SELECT count(*) FROM " + SCHEMA_NAME.toUpperCase() + "." + tableName);
      try {
        rs.beforeFirst();
        while (rs.next()) {
          nbrOfRows = rs.getDouble(1);
          logWriter.info("BackupRestoreBigDataTest.getTableRowCounts-For table '" + tableName +
                         "' we got '" + nbrOfRows + "' rows.");
          tableRowCounts.put(tableName, nbrOfRows);
        }
      } catch (SQLException e) {
        throw new TestException(TestHelper.getStackTrace(e));
      }
    }

    return tableRowCounts;
  }

  /**
   * Use this to validate all data generated during the test. Call this from a Hydra CloseTask in batch mode. It is
   * intended to run for a period of time using each thread's OpTracker to step through each key validating the correct
   * data when a row is found and validating that now row is found if the row has been deleted. Processing stops normally
   * for each Thread when all keys have been processed for it's OpTracker or if the predetermined number of validation
   * errors has been met.
   */
  @SuppressWarnings("unchecked cast")
  public static void HydraTask_validateData() {
    // Get the OpTracker for this thread
    OpTracker opTracker = (OpTracker) threadLocal_opTracker.get();
    // Make sure that the OpTracker is 'up-to-date' by doing a rebase (just once)
    int maxPK = new Long(BackupAndRestoreBB.getBB().getSharedCounters().read(BackupAndRestoreBB.largestKey)).intValue();
    if (opTracker.getMaxEndKey() != maxPK) {
      opTracker.rebaseAt(maxPK);
      logWriter.info("BackupRestoreBigDataTest.HydraTask_validateData-rebased the OpTracker to maxPK=" + maxPK +
                     " opTracker=" + opTracker.toString());
    }
    List<TableDefinition> tableList = (List<TableDefinition>) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TABLES);
    Connection conn = (Connection) threadLocal_gfxdConnection.get();
    Map<String, List> selectStmtMap = createPreparedSelectStmtMap(conn, tableList);
    int pkIndex;
    // Validate some data for some period of time
    long msToRun = (TestConfig.tab().longAt(TestHelperPrms.minTaskGranularitySec, 30) * TestHelper.SEC_MILLI_FACTOR);
    long startTime = System.currentTimeMillis();
    do {
      pkIndex = opTracker.nextKeyForValidation();
      threadLocal_opTracker.set(opTracker);
      // when all of this OpTrackers data has been validated, break-out of the loop
      if (pkIndex < 0) {
        break;
      }
      // Increment the validation counter
      BackupAndRestoreBB.getBB().getSharedCounters().increment(BackupAndRestoreBB.validateCnt);
      validateRowInAllTables(tableList, selectStmtMap, pkIndex, opTracker);

      // Check to see if we have encountered too many errors
      long validationErrorCnt = BackupAndRestoreBB.getBB().getSharedCounters().read(BackupAndRestoreBB.validationErrorCnt);
      int maxValidationErrors = BackupAndRestorePrms.getMaxValidationErrors();
      if (maxValidationErrors > 0 && validationErrorCnt >= maxValidationErrors) {
        throw new StopSchedulingTaskOnClientOrder("We have encountered too may errors. We were ask to stop validating at " +
                                                  maxValidationErrors + "errors. We have found " + validationErrorCnt +
                                                  ". Stopping any further validation.");
      }

    } while ((System.currentTimeMillis() - startTime) < msToRun);
    // when all of this OpTrackers data has been validated, stop calling this CloseTask
    if (pkIndex < 0) {
      throw new StopSchedulingTaskOnClientOrder("All data validation is finished. Stopping this clients CloseTask.");
    }
  }

  /**
   * Given a specific PK we need to validate the data in all tables and (all columns) when we are handling an insert or
   * update. If the PK has been deleted, we need to validate that the row has in fact been deleted after the restore.
   *
   * @param tableList The List of TableDefinitions in order to process each table
   * @param selectStmtMap The Map of Select statements in order to validate the data
   * @param pkIndex The int value of the key we would like to validate.
   * @param opTracker The OpTracker for this Thread - this is needed to determine if this key is being 'tracked' by the
   *                  OpTracker. Being 'tracked' means that it was not just inserted, but updated or deleted.
   */
  private static void validateRowInAllTables(List<TableDefinition> tableList, Map<String, List> selectStmtMap, int pkIndex, OpTracker opTracker) {
    // First check if we are Tracking this key (if not, it better be an INSERT)
    boolean trackingUpdate = false;
    boolean trackingDelete = false;
    if (opTracker.isKeyTracked(pkIndex)) {
      if (opTracker.isUpdateTracker()) {
        trackingUpdate = true;
      } else if (opTracker.isDeleteTracker()) {
        trackingDelete = true;
      }
    }

    String tableName, selectStmt;
    StringBuilder errorSB;
    List stmtList;
    PreparedStatement preparedStmt;
    List<Object> preparedStmtArgs;
    ResultSet resultSet;

    errorSB = new StringBuilder();
    for (TableDefinition table : tableList) {
      // Gather the data to build the Prepared Query Statement
      tableName = table.getFullTableName();
      stmtList = selectStmtMap.get(tableName);
      selectStmt = (String) stmtList.get(0);
      preparedStmt = (PreparedStatement) stmtList.get(1);
      preparedStmtArgs = buildSelectPreparedStmtArgs(table.getColumnType(0), pkIndex);
      // Execute the Prepared Query Statement
      resultSet = executePreparedStatementQuery(preparedStmt, selectStmt, preparedStmtArgs);

      try {
        // Make sure we have a row
        boolean haveARow = resultSet.first();
        if (haveARow) {
          if (trackingDelete) {
            // We have a row and we are tracking a delete, so we have an ERROR
            errorSB.append("  Table '").append(tableName)
                   .append("' contains a row that should have been deleted. We expected all rows with the key '")
                   .append(pkIndex)
                   .append("' to be deleted. We used the following query=")
                   .append(selectStmt)
                   .append("\n");
            BackupAndRestoreBB.getBB().getSharedCounters().increment(BackupAndRestoreBB.validationErrorCnt); // Increment the error counter
          } else {
            // We have a row & no delete must be a insert or update, lets validate the data
            errorSB.append(validateColumns(table, pkIndex, resultSet, trackingUpdate));
          }
        } else { // No Row
          if (trackingDelete) {
            // We don't have a row, we are tracking a delete so we shouldn't have a row, all is good
          } else {
            // No Row and Not tracking a delete, so we have an ERROR
            errorSB.append("  Table '").append(tableName)
                   .append("' did not contain a row that it should have. We were expecting a row with the key '")
                   .append(pkIndex)
                   .append("' to be found using the following query=")
                   .append(selectStmt)
                   .append("\n");
            BackupAndRestoreBB.getBB().getSharedCounters().increment(BackupAndRestoreBB.validationErrorCnt); // Increment the error counter
          }
        }
      } catch (SQLException e) {
        throw new TestException(TestHelper.getStackTrace(e));
      }
    }
    // If this PK encountered an error, save the errors for this PK to the BB
    if (errorSB.length() != 0) {
      // Prefix the constructed error string with a heading
      errorSB = new StringBuilder("Errors for primary key '")
        .append(pkIndex)
        .append("':\n")
        .append(errorSB.toString())
        .append("\n");
      // And store it
      BackupAndRestoreBB.getBB().getSharedMap().put(KEY_VALIDATION_ERRORS + pkIndex, errorSB);
    }
  }

  /**
   * This is used to validate the data that has retrieved from the restored system is correct. Each column for the table
   * is vetted against known values that should be stored for that column type based on whether this row is the original
   * insert or has been updated.
   *
   * @param table The TableDefinition for the table to be used to determine the number of columns and each column's type.
   * @param pkIndex The int value of the key we would like to validate.
   * @param resultSet The ResultSet for this PK.
   * @param isUpdate An indicator specifying if this PK has been updated or not.
   *
   * @return a String containing any errors, if any.
   */
  private static String validateColumns(TableDefinition table, int pkIndex, ResultSet resultSet, boolean isUpdate) {
    StringBuilder errorForPkSB = new StringBuilder();

    int expectedIntValue;
    String expectedStringPkValue;
    String expectedStringValue;
    String expectedDateValue;
    if (isUpdate) {
      expectedIntValue = pkIndex + 5;
      expectedStringPkValue = PREFIX_PK + pkIndex;
      expectedStringValue = PREFIX_UPDATE;
      expectedDateValue = updateDate.toString();
    } else {
      expectedIntValue = pkIndex;
      expectedStringPkValue = PREFIX_PK + pkIndex;
      expectedStringValue = PREFIX_INSERT;
      expectedDateValue = insertDate.toString();
    }
    try {
      int rsColNbr;
      for (int tblColNbr = 0;tblColNbr < table.getNumColumns();tblColNbr++) {
        rsColNbr = tblColNbr + 1;  // The ResultSet's column number is not zero-based
        String dataType = table.getColumnType(tblColNbr);
        if (DATA_TYPE_INTEGER.equalsIgnoreCase(dataType)) {
          Integer dataValue = resultSet.getInt(rsColNbr);
          if (dataValue != expectedIntValue) {
            errorForPkSB.append("  Table '").append(table.getFullTableName())
                        .append("' has the wrong value for column '").append(tblColNbr)
                        .append("'. We expected '").append(expectedIntValue)
                        .append("' , but we have '").append(dataValue)
                        .append("'.\n");
            BackupAndRestoreBB.getBB().getSharedCounters().increment(BackupAndRestoreBB.validationErrorCnt); // Increment the error counter
          }

        } else if (DATA_TYPE_VARCHAR.equalsIgnoreCase(dataType)) {
          String dataValue = resultSet.getString(rsColNbr);
          if (tblColNbr == 0) { // primary key
            if (!dataValue.equals(expectedStringPkValue)) {
              errorForPkSB.append("  Table '").append(table.getFullTableName())
                          .append("' has the wrong value for column '").append(tblColNbr)
                          .append("'. We expected '").append(expectedStringPkValue)
                          .append("' , but we have '").append(dataValue)
                          .append("'.\n");
              BackupAndRestoreBB.getBB().getSharedCounters().increment(BackupAndRestoreBB.validationErrorCnt); // Increment the error counter
            }
          } else {
            if (!dataValue.startsWith(expectedStringValue)) {
              errorForPkSB.append("  Table '").append(table.getFullTableName())
                          .append("' has the wrong value for column '").append(tblColNbr)
                          .append("'. We expected it to start with '").append(expectedStringValue)
                          .append("' , but we have '").append(dataValue)
                          .append("'.\n");
              BackupAndRestoreBB.getBB().getSharedCounters().increment(BackupAndRestoreBB.validationErrorCnt); // Increment the error counter
            }
          }
        } else if (DATA_TYPE_DATE.equalsIgnoreCase(dataType)) {
          String dataValue = resultSet.getDate(rsColNbr).toString();
          if (!dataValue.equals(expectedDateValue)) {
            errorForPkSB.append("  Table '").append(table.getFullTableName())
                        .append("' has the wrong value for column '").append(tblColNbr)
                        .append("'. We expected '").append(expectedDateValue)
                        .append("' , but we have '").append(dataValue)
                        .append("'.\n");
            BackupAndRestoreBB.getBB().getSharedCounters().increment(BackupAndRestoreBB.validationErrorCnt); // Increment the error counter
          }
        } else if (DATA_TYPE_CLOB.equalsIgnoreCase(dataType)) {
          Clob aClob = resultSet.getClob(rsColNbr);
          String clobValue = aClob.getSubString(1, expectedStringValue.length());
          if (!clobValue.equals(expectedStringValue)) {
            errorForPkSB.append("  Table '").append(table.getFullTableName())
                        .append("' has the wrong value for (Clob) column '").append(tblColNbr)
                        .append("'. We expected it to start with '").append(expectedStringValue)
                        .append("' , but we have '").append(clobValue)
                        .append("'.\n");
            BackupAndRestoreBB.getBB().getSharedCounters().increment(BackupAndRestoreBB.validationErrorCnt); // Increment the error counter
          }
          aClob.free();
        }
      }
    } catch (SQLException e) {
      throw new TestException(TestHelper.getStackTrace(e));
    }

    return errorForPkSB.toString();
  }

  /**
   * Create and return a Map containing insert statements and prepared statements per table.
   *
   * @param tableList A List of TableDefinitions, one per table
   *
   * @return A Map key: (String) fully qualified table name value: (List), at index 0: String insert statement for the
   * table at index 1: A prepared statement for the table
   */
  private static Map<String, List> createPreparedSelectStmtMap(Connection conn, List<TableDefinition> tableList) {
    //SELECT pKey, col_2 FROM bigDataTest.Table_003 WHERE pKey=?
    Map<String, List> selectStmtMap = new HashMap<String, List>();
    for (TableDefinition table : tableList) {
      // Create the select statement, suitable for a prepared statement
      StringBuilder selectStmt = new StringBuilder();
      String fullTableName = table.getFullTableName();
      selectStmt.append("SELECT ");
      int nbrColumns = table.getNumColumns();
      for (int colNbr = 0;colNbr < nbrColumns;colNbr++) {
        selectStmt.append(table.getColumnName(colNbr));
        if (colNbr != (nbrColumns - 1)) {
          selectStmt.append(", ");
        }
      }
      selectStmt.append(" FROM ").append(fullTableName).append(" WHERE pKey=?");

      try {
        // create the prepared statement
        PreparedStatement preparedStmt = conn.prepareStatement(
          selectStmt.toString(),
          ResultSet.TYPE_SCROLL_INSENSITIVE,
          ResultSet.CONCUR_READ_ONLY);
        // add to the map
        List<Object> stmtList = new ArrayList<Object>();
        stmtList.add(selectStmt.toString());
        stmtList.add(preparedStmt);
        selectStmtMap.put(fullTableName, stmtList);
      } catch (SQLException e) {
        throw new TestException(TestHelper.getStackTrace(e));
      }

    }
    logWriter.fine("BackupRestoreBigDataTest.createPreparedSelectStmtMap-Created prepared statement map: " + selectStmtMap);
    return selectStmtMap;
  }

  /**
   * Use this to generate the List of Objects to be used as arguments in PreparedStatement (executeQuery) to perform
   * A Sql Select
   *
   * @param columnType A String value for the type of column that will be use in the query's where clause
   * @param pkIndex an int that represents the argument to be used in the query
   *
   * @return A List of Objects intended to be used as arguments in a PreparedStatement (executeQuery)
   */
  private static List<Object> buildSelectPreparedStmtArgs(String columnType, int pkIndex) {
    List<Object> preparedStmtArgs = new ArrayList<Object>();

    //SELECT pKey, col_2 FROM bigDataTest.Table_003 WHERE pKey=?
    if (columnType.equalsIgnoreCase(DATA_TYPE_VARCHAR)) {
      preparedStmtArgs.add(PREFIX_PK + pkIndex);
    } else {
      preparedStmtArgs.add(pkIndex);
    }

    return preparedStmtArgs;
  }

  /**
   * Use this to execute a previously generated PreparedStatement (query)
   *
   * @param preparedStmt The previously created PreparedStatement
   * @param selectStmt The String representation of the select statement to execute
   * @param args A List of Objects to be converted and used as the arguments in the PreparedStatement
   *
   * @return The ResultSet generated by executing the query (PreparedStatement)
   */
  private static ResultSet executePreparedStatementQuery(PreparedStatement preparedStmt,
                                                         String selectStmt,
                                                         List<Object> args) {
    // Log the sql statement that would be executed by the Prepared Statement
    Object anArg;
    StringBuilder argsStr = new StringBuilder();
    for (int argCtr = 0;argCtr < args.size();argCtr++) {
      anArg = args.get(argCtr);
      argsStr.append("'").append(anArg).append("'");
      if (argCtr < (args.size() - 1)) {
        argsStr.append(", ");
      }
    }
    logWriter.fine("BackupRestoreBigDataTest.executePreparedStatementQuery-selectStmt=" + selectStmt +
                   " with arg(s): " + argsStr);
    ResultSet rs;
    try {
      // Set the values for the Prepared Statement
      int argIndex;
      Object arg;
      for (int i = 0;i < args.size();i++) {
        argIndex = i + 1;
        arg = args.get(i);
        if (arg instanceof String) {
          preparedStmt.setString(argIndex, (String) arg);
        } else if (arg instanceof Integer) {
          preparedStmt.setInt(argIndex, (Integer) arg);
        }
      }

      // Execute the Prepared Statement
      rs = preparedStmt.executeQuery();

    } catch (SQLException e) {
      throw new TestException(TestHelper.getStackTrace(e));
    }
    return rs;
  }

  /**
   * Use this to perform a restore of previously backed-up system files
   */
  public static void HydraTask_doRestoreBackup() {
    deleteExistingDiskDirs();
    performRestoreOfBackup();
  }

  /**
   * Delete existing disk directories if they contain disk files.
   */
  private static void deleteExistingDiskDirs() {
    String currDirName = System.getProperty("user.dir");
    logWriter.info("BackupRestoreBigDataTest.deleteExistingDiskDirs-currDirName=" + currDirName);

    String dirName;
    List<String> dirsToDelete = new ArrayList<String>();
    File currDir = new File(currDirName);
    File[] contents = currDir.listFiles();
    for (File aDir : contents) {
      dirName = aDir.getName();
      // Must be a '_disk' directory that is NOT from the locator
      if (aDir.isDirectory() && (dirName.endsWith(DISK_DIR_SUFFIX)) && (!dirName.contains("locator"))) {
        logWriter.fine("BackupRestoreBigDataTest.deleteExistingDiskDirs-aDir.getName()=" + dirName);
        dirsToDelete.add(dirName);
      }
    }
    int deletedCnt = 0;
    for (String dirToDelete : dirsToDelete) {
      deletedCnt += deleteDir(new File(dirToDelete));
    }
    logWriter.info("BackupRestoreBigDataTest.deleteExistingDiskDirs-deletedCnt=" + deletedCnt);
  }

  /**
   * Recursively delete a directory and its contents. Intended for use before a restore
   *
   * @param aDir The directory to delete.
   */
  private static int deleteDir(File aDir) {
    int deletedCnt = 0;
    List<String> deleteExceptions = new ArrayList<String>();

    File[] contents = aDir.listFiles();
    for (File aFile : contents) {
      if (aFile.isDirectory()) {
        deletedCnt += deleteDir(aFile);
      } else {
        if (aFile.delete()) {
          logWriter.fine("BackupRestoreBigDataTest.deleteDir-Deleted file: " + aFile.getAbsolutePath());
          deletedCnt++;
        } else {
          deleteExceptions.add("Could not delete file: " + aFile.getAbsolutePath());
        }
      }
    }

    if (aDir.delete()) {
      logWriter.fine("BackupRestoreBigDataTest.deleteDir-Deleted directory: " + aDir.getAbsolutePath());
      deletedCnt++;
    } else {
      logWriter.fine("BackupRestoreBigDataTest.deleteDir-About to delete aDir.canWrite()=" + aDir.canWrite());
      deleteExceptions.add("Could not delete directory: " + aDir.getAbsolutePath());
    }

    if (deleteExceptions.size() > 0) {
      throw new TestException("BackupRestoreBigDataTest.deleteDir-Could not delete the following directories / files: " +
                              deleteExceptions.toString());
    }

    return deletedCnt;
  }

  /**
   * Restores previously backed up disk files
   * Make sure that the disk files are cleaned up prior to calling this method
   */
  private static void performRestoreOfBackup() {
    String restoreFile = RESTORE_LINUX;
    if (HostHelper.isWindows()) {
      restoreFile = RESTORE_WINDOWS;
    }

    // Find the latest backup directory
    File latestBackupDir = findLatestBackupDir();
    if (latestBackupDir == null) {
      throw new TestException("Expecting the test directory to contain at least 1 backup directory, but none were found.");
    }

    // Check the contents of the backup directory
    for (File hostVMDir : latestBackupDir.listFiles()) {
      // Script the restoring of the locator files
      logWriter.fine("BackupRestoreBigDataTest.performRestoreOfBackup-hostVMDir.getName()=" + hostVMDir.getName());
      if (hostVMDir.getName().contains("locator")) {
        continue;
      }
      // Check the contents of the host's backup directories
      for (File aFile : hostVMDir.listFiles()) {
        logWriter.fine("BackupRestoreBigDataTest.performRestoreOfBackup-aFile.getName()=" + aFile.getName());
        // Find the restore script
        if (aFile.getName().equals(restoreFile)) {
          // Run the script
          try {
            String command = aFile.getCanonicalPath();
            logWriter
              .info("BackupRestoreBigDataTest.performRestoreOfBackup-Running restore scripts, command=" + command);
            String cmdResult = ProcessMgr.fgexec(command, 5000);
            logWriter
              .info("BackupRestoreBigDataTest.performRestoreOfBackup-Restore script executed successfully. cmdResult=" + cmdResult);
          } catch (HydraRuntimeException e) {
            throw e;
          } catch (IOException e) {
            throw new TestException(TestHelper.getStackTrace(e));
          }
          break;
        }
      }
    }
  }

  /**
   * Use this to locate the latest backup directory created during the backup process
   *
   * @return File of the last directory created during a backup operation. Null if none are found
   */
  private static File findLatestBackupDir() {
    File backupDir = null;
    // Get the latest backup number
    long lastBackupNbr = BackupAndRestoreBB.getBB().getSharedCounters().read(BackupAndRestoreBB.BackupCtr);
    logWriter.info("BackupRestoreBigDataTest.findLatestBackupDir-lastBackupNbr=" + lastBackupNbr);

    // Find the backup directories
    File rootBackupDir = new File(BackupAndRestorePrms.getBackupPath());
    logWriter.info("BackupRestoreBigDataTest.findLatestBackupDir-rootBackupDir=" + rootBackupDir);

    // Check the contents of the current test directory
    for (File aDir : rootBackupDir.listFiles()) {
      // Find the last backup directory
      if (aDir.getName().equalsIgnoreCase(BACKUP_PREFIX + lastBackupNbr)) {
        // Get the contents of this backup directory
        File[] backupContents = aDir.listFiles();
        // We should have exactly one directory
        if (backupContents == null || backupContents.length != 1) {
          throw new TestException("Expecting backup directory to contain 1 directory, but it " +
                                  (backupContents == null ? "is null." : "contains " + backupContents.length));
        }
        // Use this as the latest backup directory
        backupDir = backupContents[0];
        break;
      }
    }
    logWriter.info("BackupRestoreBigDataTest.findLatestBackupDir-Backup dir is " + backupDir);
    return backupDir;
  }

  /**
   * Use this to display a test report. Report contains various counters kept during the test and all the errors
   * collected during validation.
   */
  @SuppressWarnings("unchecked cast")
  public static void HydraTask_testDataReport() {
    SharedCounters sharedCounters = BackupAndRestoreBB.getBB().getSharedCounters();

    // Log a nice report on the data generated and validated
    long totalBytes = sharedCounters.read(BackupAndRestoreBB.totalDataBytes);
    float totalMB = (totalBytes / (1024f * 1024f));
    float totalGB = (totalBytes / (1024f * 1024f * 1024f));
    long backupCtr = sharedCounters.read(BackupAndRestoreBB.BackupCtr);
    long largestKey = sharedCounters.read(BackupAndRestoreBB.largestKey);
    int nbrTables = BackupAndRestorePrms.getNbrTables();
    long nbrOfUpdates = sharedCounters.read(BackupAndRestoreBB.nbrOfUpdates) / nbrTables;
    long nbrOfDeletes = sharedCounters.read(BackupAndRestoreBB.nbrOfDeletes) / nbrTables;

    // Build the lob string - if needed
    String lobInfo = "";
    int nbrLobColumns = BackupAndRestorePrms.getNbrLobColumns();
    if (nbrLobColumns > 0) {
      String lobDataType = BackupAndRestorePrms.getLobColumnType();
      lobInfo = "\n  - Spread among these tables are " + nbrLobColumns + " " + lobDataType +
                " column(s) of size(s): ";

      int nbrColumns;
      String dataType, clobSizes = "";
      List<TableDefinition> tableList = (List<TableDefinition>) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TABLES);
      for (TableDefinition table : tableList) {
        nbrColumns = table.getNumColumns();
        for (int colNbr = 0;colNbr < nbrColumns;colNbr++) {
          dataType = table.getColumnType(colNbr);
          if (DATA_TYPE_CLOB.equalsIgnoreCase(dataType)) {
            if (clobSizes.length() > 0) {
              clobSizes += ", ";
            }
            clobSizes += table.getColumnLength(colNbr);
          }
        }
      }
      lobInfo += clobSizes + ".";
    }

    StringBuilder infoSB = new StringBuilder("BackupRestoreBigDataTest.HydraTask_testDataReport-We are done with the test.");
    infoSB.append(" Here are the results:\n  - We generated ")
          .append(totalMB)
          .append(" MB of data (")
          .append(totalGB)
          .append(" GB)\n  - We performed ")
          .append(backupCtr)
          .append(" backups.\n  - We inserted ")
          .append(largestKey)
          .append(" rows into ")
          .append(nbrTables)
          .append(" tables (")
          .append(largestKey * nbrTables)
          .append(" total rows among all tables).")
          .append(lobInfo)
          .append("\n  - On those rows we performed ")
          .append(nbrOfUpdates)
          .append(" updates and ")
          .append(nbrOfDeletes)
          .append(" deletes.");

    // Calculate and Report the duration of the Data Initialization step
    long start = (Long) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TIMINGS + "-dataInitialization-Start");
    long stop = (Long) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TIMINGS + "-dataInitialization-Stop");
    long duration = stop - start;
    infoSB.append("\n  - The Data Initialization task took ").append(reportTimeDuration(duration)).append(" for ~")
          .append(BackupAndRestorePrms.getInitialDataMB()).append(" MB of data.");

    // Calculate and Report the duration of the Perform Random Operations step
    start = (Long) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TIMINGS + "-randomOps-Start");
    stop = (Long) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TIMINGS + "-randomOps-Stop");
    duration = stop - start;
    infoSB.append("\n  -   The Random Operations task took ").append(reportTimeDuration(duration)).append(" for ~")
          .append(BackupAndRestorePrms.getDesiredDataOpsMB()).append(" MB of data.");

    // Calculate and Report the duration of the Data Restoration step
    start = (Long) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TIMINGS + "-dataRestoration-Start");
    stop = (Long) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TIMINGS + "-dataRestoration-Stop");
    duration = stop - start;
    infoSB.append("\n  -    The Data Restoration task took ").append(reportTimeDuration(duration)).append(".");

    // Calculate and Report the duration of the Restart GemFireXD step
    start = (Long) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TIMINGS + "-restartGemFireXD-Start");
    stop = (Long) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TIMINGS + "-restartGemFireXD-Stop");
    duration = stop - start;
    infoSB.append("\n  -   The Restart GemFireXD task took ").append(reportTimeDuration(duration)).append(".");

    // Calculate and Report the duration of the Data Validation step
    start = (Long) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TIMINGS + "-dataValidation-Start");
    stop = (Long) BackupAndRestoreBB.getBB().getSharedMap().get(KEY_TIMINGS + "-dataValidation-Stop");
    duration = stop - start;
    long validateCnt = sharedCounters.read(BackupAndRestoreBB.validateCnt);
    infoSB.append("\n  -     The Data Validation task took ").append(reportTimeDuration(duration)).append(" on ")
          .append(validateCnt).append(" rows.");

    long validationErrorCnt = sharedCounters.read(BackupAndRestoreBB.validationErrorCnt);
    infoSB.append("\n  - During validation, we found ").append(validationErrorCnt).append(" errors.");

    // If we were ask to stop validating after so many errors, let's report that
    int maxValidationErrors = BackupAndRestorePrms.getMaxValidationErrors();
    if(maxValidationErrors > 0 && validationErrorCnt > 0) {
      infoSB.append("\n  - Even though we found errors, we were asked to stop validating at ")
            .append(maxValidationErrors).append(" errors.");
    }

    Log.getLogWriter().info(infoSB.toString());

    // Get and report all of the accumulated validation errors.
    StringBuilder errorSB = new StringBuilder();
    for (Object e : BackupAndRestoreBB.getBB().getSharedMap().getMap().entrySet()) {
      Map.Entry entry = (Map.Entry) e;
      String key = (String) entry.getKey();
      if (key.startsWith(KEY_VALIDATION_ERRORS)) {
        errorSB.append((StringBuilder) BackupAndRestoreBB.getBB().getSharedMap().get(key));
      }
    }
    if (errorSB.length() != 0) {
      throw new TestException("Error(s)!\n" + errorSB.toString());
    }
  }

  /**
   * Use this to convert an elapsed time (in nanoseconds) to a readable String
   *
   * @param timeDuration A long representing the elapsed time (in nanoseconds) to be converted to a readable String.
   *
   * @return A String in readable time format (HH:MM:SS.mmm).
   */
  private static String reportTimeDuration(long timeDuration) {
    Log.getLogWriter().info("BackupRestoreBigDataTest=reportTimeDuration-timeDuration=" + timeDuration);

    // Calculate any hours in the time duration. Remove the equivalent hours from the remaining time duration.
    long elapsedHours = TimeUnit.NANOSECONDS.toHours(timeDuration);
    if (elapsedHours > 0) {
      timeDuration = timeDuration - (TimeUnit.HOURS.toNanos(elapsedHours));
    }

    // Calculate any minutes in the time duration. Remove the equivalent minutes from the remaining time duration.
    long elapsedMinutes = TimeUnit.NANOSECONDS.toMinutes(timeDuration);
    if (elapsedMinutes > 0) {
      timeDuration = timeDuration - (TimeUnit.MINUTES.toNanos(elapsedMinutes));
    }

    // Calculate any seconds in the time duration. Remove the equivalent seconds from the remaining time duration.
    long elapsedSeconds = TimeUnit.NANOSECONDS.toSeconds(timeDuration);
    if (elapsedSeconds > 0) {
      timeDuration = timeDuration - (TimeUnit.SECONDS.toNanos(elapsedSeconds));
    }

    // Calculate any milliseconds in the time duration.
    long elapsedMillis = TimeUnit.NANOSECONDS.toMillis(timeDuration);

    return String.format("%02d:%02d:%02d.%d", elapsedHours, elapsedMinutes, elapsedSeconds, elapsedMillis);
  }

}
